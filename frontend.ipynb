{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9deiVbD0zN-8",
        "outputId": "57ca81f3-e173-44ff-824a-bbaeb1b98cfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app_code = app_code = \"\"\"\n",
        "import streamlit as st\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from peft import PeftModel, PeftConfig\n",
        "import torch\n",
        "\n",
        "st.set_page_config(page_title=\"ðŸ’¬ Fine-Tuned TinyLLaMA LLM Chat\", layout=\"wide\")\n",
        "st.title(\"ðŸ’¬ Fine-Tuned TinyLLaMA LLM Chat\")\n",
        "\n",
        "# Path to your unzipped LoRA adapter\n",
        "ADAPTER_PATH = \"/content/dpo-trial3/dpo-trial3\"\n",
        "#ADAPTER_PATH = \"/content/sft-trial4/sft-trial4\"\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    # Load adapter config\n",
        "    config = PeftConfig.from_pretrained(ADAPTER_PATH)\n",
        "\n",
        "    # Load base model\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        config.base_model_name_or_path,\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "    # Apply LoRA adapter\n",
        "    model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
        "\n",
        "    # Create text generation pipeline\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=512,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,\n",
        "        do_sample=False,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.9,\n",
        "        return_full_text=True,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    return pipe, tokenizer\n",
        "\n",
        "generator, tokenizer = load_model()\n",
        "\n",
        "prompt = st.text_area(\"Enter your prompt:\", height=150)\n",
        "\n",
        "if st.button(\"Generate Response\"):\n",
        "    with st.spinner(\"Generating...\"):\n",
        "        try:\n",
        "            # Format prompt\n",
        "            formatted_prompt = f\"### Instruction:\\\\n{prompt}\\\\n\\\\n### Response:\"\n",
        "            output = generator(formatted_prompt)[0][\"generated_text\"]\n",
        "            response_only = output.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "            # Clean output (optional)\n",
        "            def clean_output(text):\n",
        "                return text.split(\"###\")[0].split(\"\\\\n\")[0].strip()\n",
        "\n",
        "            st.markdown(\"**Response:**\")\n",
        "            st.markdown(response_only.replace(\"\\\\n\", \"  \\\\n\"))\n",
        "\n",
        "            st.write(clean_output(response_only))\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Generation error: {str(e)}\")\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_code)\n"
      ],
      "metadata": {
        "id": "fXZKlX8mzYFq"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip -o dpo-trial3_safe.zip -d /content/dpo-trial3"
      ],
      "metadata": {
        "id": "N-3Zy3-6zzKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/log.txt &"
      ],
      "metadata": {
        "id": "S6qCQrZA0ojI"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "print(\"Tunnel password (public IP):\", requests.get(\"https://loca.lt/mytunnelpassword\").text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyw8IBzw4eij",
        "outputId": "4e4f8d8a-386e-4713-f0d3-5e7c57f8a6b0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tunnel password (public IP): 34.21.93.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmyKkTtM4d0P",
        "outputId": "fa653151-6ac3-4427-b8c3-58f03e87da13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kyour url is: https://cool-wombats-knock.loca.lt\n"
          ]
        }
      ]
    }
  ]
}